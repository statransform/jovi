## Defining the null hypothesis of interest {#interpretation}
Before comparing different statistical methods, it is essential to assess whether they are actually comparable. If two methods are not designed to test the same null hypothesis, then direct comparisons between them can be misleading. In what follows, we clarify the interpretation of main and interaction effects and explain how we address potential issues in our analysis.

### Interpreting main effects
The traditional ANOVA is used to test differences between two or more means. However, nonparametric tests often target other population parameters. For example, the Wilcoxon sign-rank test is commonly described as a test of medians for paired samples [@McDonald:2014] and is used when population means are not of interest, e.g., when population distributions are skewed. The Mann-Whitney U and the Kruskal–Wallis tests are used, instead, to assess whether two or more independent samples come from the same population, or more technically, whether the mean ranks of the groups are the same. They can be only interpreted as tests of medians under the strict assumption that the population distributions of all groups have identical shapes and scales [@Divine:2018]. 

Defining the null hypothesis of interest of a rank transformation is more challenging. @conover:1981 show that the simple rank transformation procedure (RNK) is equivalent to the Mann-Whitney U and Kruskal–Wallis tests for independent samples. For paired samples, however, it results in a new test, which is different from the Wilcoxon sign-rank test and the Friedman test. Defining the null hypothesis of interest of ART is even more challenging because of the hybrid nature of the method. In particular, while ART is a rank-based transformation procedure, it aligns data with respect to means, where alignment is performed independently for each group.

**Dealing with interpretation issues.** To avoid such interpretation issues, we focus --- unless otherwise stated --- on effects that apply monotonic transformations to population distributions. This also ensures a monotonic relationship between different measures of central tendency, such as medians and means (with the exception of the Cauchy distribution, where the mean is undefined). In other words, if a treatment increases the population mean, it will also increase the population median. We present an example in @fig-distributions. The figure shows two population distributions corresponding to the two intermediate levels of difficulty of our illustrative example (see @fig-example). We observe that the increased difficulty of the task translates both the population mean and the median to the right. In this case, we expect a statistical test to reject the null hypothesis, no matter whether it tests the population mean, the median, or the overall distribution shape.   

::: {#fig-distributions}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 9, warning=FALSE}
library(plotly)

palette <- c("#E69F00", "#009E73")

logsd <- 0.5
logm1 <- -0.3
logm2 <- 0.3

xs <- seq(0, 10, length.out = 1000)
y1s <- dlnorm(xs, logm1, logsd)
y2s <- dlnorm(xs, logm2, logsd)

m1 <- exp(logm1 + (logsd^2)/2)
m2 <- exp(logm2 + (logsd^2)/2)
med1 <- exp(logm1)
med2 <- exp(logm2)

anot_m1 <- list(x = m1, y = 0, text = "mean", xref = "x", yref = "y",
  showarrow = TRUE, arrowhead = 6, arrowsize = 1, arrowcolor = palette[1],
  ax = 0, ay = -40
)

anot_med1 <- list(x = med1, y = 0, text = "median", xref = "x", yref = "y", 
  showarrow = TRUE, arrowhead = 7, arrowsize = 1, arrowcolor = palette[1],
  ax = 0, ay = 30
)

anot_m2 <- list(x = m2, y = 0, text = "mean", xref = "x", yref = "y",
  showarrow = TRUE, arrowhead = 6, arrowsize = 1, arrowcolor = palette[2],
  ax = 0, ay = -43
)

anot_med2 <- list(x = med2, y = 0, text = "median", xref = "x", yref = "y", 
  showarrow = TRUE, arrowhead = 7, arrowsize = 1, arrowcolor = palette[2],
  ax = 0, ay = 30
)

annot1 <- paste("Log-normal: meanlog = ", logm1, ", sdlog = ", logsd)
annot2 <- paste("Log-normal: meanlog = ", logm2, ", sdlog = ", logsd)

fig <- plot_ly() %>%  
      add_lines(x = xs, y = y1s, visible = TRUE, type = 'scatter', mode = 'lines', text = annot1, hoverinfo = 'text', line=list(color=palette[1])) %>%
      add_lines(x = xs, y = y2s, visible = TRUE, type = 'scatter', mode = 'lines', text = annot2, hoverinfo = 'text', line=list(color=palette[2])) %>%      
      add_annotations(x = med1, y = max(y1s), text = "Difficulty Level 2", xref = "x", yref = "y", showarrow = F, font = list(color = palette[1]), xanchor = 'left') %>% 
      add_annotations(x = med2, y = max(y2s), text = "Difficulty Level 3", xref = "x", yref = "y", showarrow = F, font = list(color = palette[2]), xanchor = 'left') %>% 
      layout(annotations = anot_m1) %>% layout(annotations = anot_med1) %>%      
      layout(annotations = anot_m2) %>% layout(annotations = anot_med2) %>%    
      layout(
        showlegend = FALSE,
        xaxis = list(title = "Time (min)", range = c(0, 4.5), showgrid = F, showticks = T, ticks="outside", zeroline = F), 
        yaxis = list(showgrid = F, showticklabels = F, showline = F, range=c(0, max(y1s) + .05), fixedrange=T)
      ) %>% 
	      config(displayModeBar = TRUE, scrollZoom = FALSE, displaylogo = FALSE, modeBarButtonsToRemove = c("lasso2d", "select2d",  "zoomIn2d", "zoomOut2d", "autoscale", "hoverclosest", 'hoverCompare'))  %>% layout(dragmode='pan')

fig

```
Time distributions for two task populations with difficulty levels 2 and 3 (see @fig-example). 
:::

### Interpreting interaction effects
The ART procedure was proposed as an alternative to the rank transformation [@conover:1981] for testing interactions. As @higgins:1990 explained, the rank transformation is non-linear and, as a result, it changes the structure of interactions. Therefore, *"interaction may exist in the transformed data but not in the original data, or vice versa"* [@higgins:1990]. @fig-interactions-rank demonstrates the problem. In this example, the data have been sampled from perfectly normal distributions with equal variances. We observe that while no interaction effect appears in the original data (lines are parallel), the rank transformation deforms the trend. In particular, differences are more pronounced for the middle points of the three-level factor ("medium difficulty"). The figure also shows that the inverse normal transformation also deforms the interaction but to a lesser extent. Note that the problem emerges when the main effect is strong on all interacting factors. 

::: {#fig-interactions-rank}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 7, warning=FALSE}
library("ARTool")

INT <- function(x){
	qnorm((rank(x) - 0.5)/length(x))
}

df <- read.csv("interactions_rank.csv", sep=",", header=TRUE, strip.white=TRUE)
df$Difficulty <- ordered(df$Difficulty, levels = c("easy", "medium", "hard"))

df_aggr <- aggregate(Time ~ Difficulty+Technique, data = df, mean)
df_rank <- aggregate(rank(Time) ~ Difficulty+Technique, data = df, mean)
df_int <- aggregate(INT(Time) ~ Difficulty+Technique, data = df, mean)
colnames(df_rank) <- c("Difficulty", "Technique", "Time")
colnames(df_int) <- c("Difficulty", "Technique", "Time")

fig1 <- createRankInteractionPlot(df_aggr) %>% layout( yaxis = list(hoverformat = '.2f'))
fig2 <- createRankInteractionPlot(df_rank, rnkscale = TRUE) %>% layout( yaxis = list(hoverformat = '.2f'))
fig3 <- createRankInteractionPlot(df_int) %>% layout( yaxis = list(title = 'INT(Time)', range = c(-2.1,2.1), hoverformat = '.2f'))

fig <- subplot(fig1, fig2, fig3, titleY = TRUE, titleX = TRUE, margin = 0.05, widths = c(0.32, 0.32, 0.32)) %>% 
	        layout(margin = list(l = 40, r = 0, b = 0, t = 50, pad = 0)) %>% layout(hovermode = 'x', dragmode='pan')

fig
```
Visualization of interaction effect for a 3 $\times$ 2 experimental design before and after applying the rank transformation and the inverse normal transformation (INT) on an [example dataset](interactions_rank.csv) (within-subject design, $n = 20$). All data points represent means.
:::

ART aims to correct this problem. However, non-linear transformations come into place in various ways in experimental designs [@Loftus:1978; @Wagenmakers:2012]. They can deform distributions, making the interpretation of observed effects especially challenging. Before presenting our experimental method, we discuss these problems and explain how our approach takes them into consideration. 

**Removable interactions.** Let us take a different [dataset](removable_interactions.csv) from a fictional experiment (within-subject design with $n = 24$) that evaluates the performance of two techniques (*Tech A* and *Tech B*) under two task difficulty levels (*easy* vs. *hard*). The experiment, for example, could test a mobile typing task, where the levels of difficulty correspond to texts of different lengths (*short* vs. *long*) under two typing techniques (*with* vs. *without auto-completion*). We assume that the researchers measure two dependent variables: task-completion time and perceived performance, which is measured through a five-level ordinal scale (from "very quick" to "very slow"). In this example, the main effects of task difficulty and technique are large. It is less clear, however, whether there is also an interaction between the two factors. 

@fig-interactions visualizes the means for each combination of the levels of the factors and highlights the possible interactions. Let us first concentrate on the first two plots that present results for the time measure. The trends in the left plot indicate an interaction effect, since the two lines seem to diverge as the task difficulty increases. 

::: {#fig-interactions}
```{r, echo=FALSE, message=FALSE, fig.height=3.5, fig.width = 9, warning=FALSE}
df <- read.csv("removable_interactions.csv", sep=",", header=TRUE, strip.white=TRUE)
dftime <- aggregate(Time ~ Difficulty+Technique, data = df, mean)
dfpref <- aggregate(PerceivedPerformance ~ Difficulty+Technique, data = df, mean)

fig1 <- createInteractionPlot(dftime) %>% layout( yaxis = list(hoverformat = '.2f'))
fig2 <- createInteractionPlot(dftime, logscale = TRUE) %>% layout( yaxis = list(hoverformat = '.2f'))
fig3 <- createInteractionPlot(dfpref, likert = TRUE) %>% layout( yaxis = list(hoverformat = '.1f'))
#fig <- subplot(fig1, fig2, fig3, titleY = TRUE, titleX = TRUE, margin = 0.08, widths = c(0.32, 0.36, 0.32))

fig <- subplot(fig1, fig2, titleY = TRUE, titleX = TRUE, margin = 0.06) %>% subplot(fig3, titleY = TRUE, titleX = TRUE, margin = 0.07, widths = c(0.66, 0.33)) %>% layout(hovermode = 'x', dragmode='pan')

fig
```
The line charts visualize the effects of task difficulty (*easy* vs. *hard*) and technique (*Tech* A vs. *Tech B*) for two measures: task completion time (left and middle) and perceived performance (right). All data points represent group means. 
:::

But how meaningful is this interpretation of interaction? As we dicussed earlier, time measurements are often taken from skewed distributions where the variance is not constant. Therefore, large effects are harder to observe in quick tasks than in slow ones. However, such trends do not necessarily reveal any real interactions, because they are simply due to observations at different time scales. @fig-interactions (middle) visualizes the effects using a logarithmic scale. Notice that the lines in the plot are now almost parallel, suggesting no interaction effect.

The concept of *removable* or *uninterpretable* interactions, that is, interactions that disappear after applying a monotonic non-linear transformation, was introduced by @Loftus:1978.  Over three decades later, @Wagenmakers:2012 revisited this work and found that psychology researchers are largely unaware of the concept, drawing incorrect conclusions about psychological effects on the basis of meaningless interactions. 

This issue also extends to data collected from questionnaires. The right plot in @fig-interactions shows results for perceived performance. Again, the line trends suggest an interaction effect. Unfortunately, the scale is ordinal, which means that distances between the five levels of the scale may not be perceived as equal by people. Furthermore, the scale is bounded, so the reason that the two lines are not parallel might be simply due to the absence of additional levels beyond the extreme "very slow" ranking. Concluding that there is a meaningful interaction here could be incorrect. @Liddell:2018 extensively discuss how ordinal scales deform interactions.

**Formal testing.** We now formally test the above interactions by using ANOVA with different transformation methods. Below, we present the *p*-value returned by each method for task-completion time: 

| PAR  | LOG  | ART | RNK | INT |
|------|------|-----|-----|-----|
| $.023$ | $.67$ | $.00073$ | $.66$ | $.67$ |
: *p*-values for interaction effect on task-completion time {.sm}

We observe that *RNK* and *INT* lead to *p*-values very close to the *p*-value of *LOG*, which suggests a similar interpretation of interaction effects. In contrast, ART returns a very low *p*-value (lower than the *p*-value of the regular ANOVA), indicating a different interpretation.  

We also test the interaction effect on the ordinal dependent variable:

| PAR  | ART | RNK | INT | ATS |
|------|-----|-----|-----|-----|
| $.0020$ | $.00075$ | $.0067$ | $.0037$ | $.0081$ |
: *p*-values for interaction effect on perceived performance {.sm}

Notice that we omit the log-transformation method (*LOG*), as it is not relevant in this context. Instead, we conduct an analysis with the nonparametric ATS method [@Brunner_ATS:2001], as implemented in the R package *nparLD* [@nparLD]. All *p*-values are low, suggesting that an interaction effect exists. However, if we conduct a more appropriate analysis using an ordered probit model [@Burkner:2019;@Christensen2023ordinal], we find no supportive evidence for such an effect (check our analysis in the supplementary material). 

In conclusion, focusing solely on the issues with the rank tranformation illustrated in @fig-interactions-rank is akin to missing the forest for the trees. When parallel main effects are present, interpreting interactions can be challenging for all methods --- and ART offers no solution to this problem. 

### Disagreement on the interpretation of effects in GLMs
We further discuss interpetation issues in the broader context of generalized linear models (GLMs).

GLMs are defined by a link function $g$ that connects a variable --- linearly defined by the predictors --- to a response variable $Y$, which follows an arbitrary distribution. For an experimental design with two factors, $x_1$ and $x_2$, the expected value (or mean) of $Y$ conditional on $x_1$ and $x_2$ is expressed as follows:

$$
E[Y|x_1, x_2] = g^{-1}(a_0 + a_1 x_1 + a_2 x_2 + a_{12}x_1 x_2)
$${#eq-glm}

where $g^{-1}$ is the inverse of the link function.

Statistical procedures based on GLMs define effects in terms of the coefficients $a_{1}$, $a_{2}$, and $a_{12}$ in the linear component of the model. That is, the null hypothesis should be rejected when the corresponding coefficient (e.g., the coefficient $a_{12}$ for the interaction) is non-zero. However, there is no consensus among researchers on this approach. We clarify the debate and explain how we address it.

**Defining interactions on the scale of the response variable.** @Ai:2003 and, more recently, @McCabe:2022 show that the interaction coefficient alone is not sufficient to describe the actual interaction trend on the natural scale of the response variable. They argue that an interaction effect should instead be defined as the observed change in the marginal effect of $x_1$ as a function of $x_2$. Thus, the trends shown in @fig-interactions do represent interactions, since the difference between *Tech* A and *Tech B* changes as a function of *Difficulty* on the original reponse scale (time or perceived performance). 

Using this definition, the authors show that a zero interaction coefficient ($a_{12} = 0$) does not necessarily imply the absence of an interaction effect. In fact, for continuous predictors, they demonstrate that when $a_{12} = 0$, the interaction effect can still emerge and is proportional to $a_1$ and $a_2$.

<!--
For continuous predictors, this can be formalized as the second-order cross partial derivative of the expected value (which is $\frac{\partial^2 E(Y|x_1, x_2)}{\partial x_1 \partial x_2}$), while for discrete predictors, it can be expressed using discrete differences between levels of the predictors. -->

**Responding to @Ai:2003.** @Greene:2010 responds that the interpretation of @Ai:2003 *“produces generally uninformative and sometimes contradictory and misleading results”* and instead advocates for using the model’s coefficients as the basis for inference:

> *“Statistical testing about the model specification is done at this step. Hypothesis tests are about model coefficients and about the structural aspects of the model specifications. Partial effects are neither coefficients nor elements of the specification of the model. They are implications of the specified and estimated model”* [@Greene:2010, Page 295].

Our position is fully aligned with Greene's argument. As our earlier example (see @fig-interactions) demonstrates, interactions assessed on the scale of the response variable can be misleading about the true underlying effects that generated the data. How meaningful is it to declare interaction effects that arise solely from parallel main effects and lack any theoretical interpretation? It is worth noting that @Ai:2003, as well as @McCabe:2022, overlook Loftus' [-@Loftus:1978] critique of uninterpretable interactions. 

Nevertheless, in most of our experiments we select scenarios that avoid these interpretation problems, ensuring that the Type I error rates of the methods we compare do not depend on how one defines interactions. We clarify this point below. 

**Clarifying when the interpretation of effects becomes ambiguous.** In nonlinear models, ambiguities in interpreting interaction effects arise when all relevant main effects are present --- that is, when the coefficients of all interacting factors are non-zero. Interpretation issues may also extend to main effects when the interaction coefficient $a_{12}$ is non-zero. In such cases, the marginal effect of a factor (e.g., $x_1$) may vary across levels of another factor even when its associated coefficient (e.g., $a_1$) is zero. 

By contrast, interpretation is unambiguous in the following siutations:

1. *Main effects*, when the interaction coefficient $a_{12}$ is zero. In this case, if $a_1 = 0$, then the factor $x_1$ has no effect on the response scale. Likewise, if $a_2 = 0$, then the factor $x_2$ has no effect on the response scale.

2. *Interaction effect*, when either $a_{1}$ or $a_{2}$ is zero. In this case, if $a_{12}=0$, then there is no interaction between $x_1$ and $x_2$ on the response scale.    

Proofs of these statements are provided in [Appendix II](appendix_II.html#spurious-absence). In these scenarios, error rates are comparable across methods, regardless of whether the null hypothesis is defined in terms of the coefficients of the linear model or the observed effects on the response scale. For example, the divergence in ART's results in our illustrative example (see @fig-example) cannot be attributed to interpretation issues, since the sample was drawn from a population with no effect of *Technique* ($a_{1} = 0$) and no interaction ($a_{12}=0$). 

Throughout the remainder of the article, we explicitly distinguish between situations in which effect interpretation is unambiguous and those in which it is not. Because different methods may rely on different assumptions about the null hypothesis, we analyze ambiguous cases separately. However, we emphasize that the existing literature offers no guidance on how ART should be expected to behave when the definition of a main or interaction effect is ambiguous. Early evaluations of ART focused on linear models. Although @elkin:2021 consider nonlinear models, they do not address how main and interaction effects should be interpreted when the definition of the null hypothesis depends on the scale of responses.
