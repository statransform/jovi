## Recommendations {#recommendations}
Reflecting on our findings, we formulate a set of recommendations that we hope will be helpful to researchers.

### Abandoning ART
Our results corroborate Lüpsen's [-@luepsen:2017; -@luepsen:2018] warnings, overwhelmingly demonstrating that the ART method is defective. ART's advantages over the simple rank transformation (RNK) are marginal and only applicable to normal distributions with equal variances. However, the method introduces additional risks for a range of non-normal distributions. Specifically, it inflates the Type I error rate of both main effects and interactions. Furthermore, the risks associated with ART are more severe compared to those associated with parametric ANOVA. While skewed distributions lead to a loss of statistical power with parametric ANOVA, ART consistently inflates Type I error rates. It is also important to emphasize that parametric ANOVA is consistently superior to ART for the analysis of ordinal data, irrespective of whether the thresholds of the ordinal scale are equidistant or not. In light of these findings, we conclude that ART is an outdated procedure and urge researchers to discontinue its use.
 
Some researchers may be tempted to seek alternative formulations of ART. In particular, @Salter:1993 report that employing ART with median-based alignment (i.e., using medians instead of means to align values) for interactions corrects the method's instability under the Cauchy distribution. We present results regarding the method's performance for the $4 \times 3$ repeated-measures design in our [appendix](appendix.html). We observe that indeed, using a median-based alignment for interactions brings error rates close to nominal levels, provided that main effects remain below a certain threshold. This trend holds across all distributions tested. However, despite this improvement, the method still lags behind INT in performance, and compared to parametric ANOVA, its advantages are evident only for the Cauchy distribution. Additionally, median-based alignment proves inappropriate for main effects, as in most cases, it yields even higher Type I error rates than mean-based alignment.

### Adopting other nonparametric methods
Given the widespread adoption of ART, many researchers may feel perplexed about which alternative nonparametric method to use. Our results indicate that INT performs well across various configurations and emerges as the top performer among rank transformations. However, like other rank-based methods, INT faces challenges in accurately inferring interactions when parallel main effects are present. It may also lead to inflated error rates or reduced power in cases of unequal variances, and tends to underestimate large effect sizes. These issues are exacerbated when dealing with discrete distributions. @Beasley:2009 identify additional situations where INT may fail, such as in highly imbalanced designs.

Despite these limitations, INT outperforms parametric ANOVA across various scenarios that we investigated. 
This is especially evident when distributions are skewed. In these cases, parametric ANOVA experiences a significant loss of power and becomes sensitive to scale interpretation issues, which typically emerge when testing interactions or when variances are unequal. 
Finally, while it is still feasible to use common one-factor nonparametric tests (e.g., the Kruskal–Wallis test, the Wilcoxon sign-rank test, and the Friedman test) independently on each factor of a multifactorial design, this approach necessitates data aggregation, which compromises the statistical power of these tests. 
Nevertheless, we found that even for one-factor designs, INT turns out to be more powerful than these nonparametric tests. 

Consequently, our recommendation aligns with Gelman's [-@gelman2015int_vs_wilcoxon] advice to use INT as a general nonparametric method, with careful consideration of potential cases of failure and a cautious interpretation of interaction effects. 

### Priviledging parametric methods
The scope of rank-based nonparametric statistics is very limited. @baguley2012 summarizes a fundamental concern regarding their use as follows: 

> *"rank transformations dont's just transform the data, they do so in a way that makes it impossible to recover the original scores ... This won't matter if the sole aim is to construct a significance test, but if you want to make predictions or obtains CIs, use of ranks introduces a potentially unnecessary obstacle."* [@baguley2012, pp. 356]

We also explained that nonparametric tests are not free of assumptions, while rank transformations complicate the interpretation of the null hypothesis. Therefore, there are valid reasons to prioritize parametric methods and reserve rank transformations as last-resort solutions. Merely obtaining a $p$-value $< .05$ from a normality test (or a test of the homogeneity of variance assumption) is insufficient justification for switching to a nonparametric procedure. According to @baguley2012, using a significance test to evaluate the statistical assumptions *"is asking completely the wrong question"* because *"It is not the presence of a violation, but the degree of departure from the assumption (i.e., effect size) that matters"* [@baguley2012, pp. 324]. 

Several papers we reviewed, citing ARTool [@wobbrock:2011], justify their use of ART based on the presence of *"nonparametric data."* 
However, it is important to clarify that terms like "parametric" or "nonparametric" refer to properties of the statistical model, not inherent characteristics of the collected data.
Even when ANOVA is clearly unsuitable, there are often alternative parametric methods for addressing violations of common statistical assumptions and still ensure that results can be easily communicated and interpreted. 
For example, one can log-transform positively skewed data following log-normal distributions or use generalized mixed-effects models, which provide support for a broader set of data distributions. 
Finally, employing simulations, as we did in this article, is an excellent method for evaluating the impact and risks of assumption violations and determine which data analysis procedures to use. 

### Analyzing Likert-type data
The analysis of Likert-type data requires special attention due to the contradicting research literature on this topic. 20 years ago, @Jamieson:2004likert_abuse alarmed researchers regarding the frequent use of analyses that treat Likert scales as interval rather than ordinal data. Jamieson's article provoked a number of vivid responses. @Carifio:2008 replied that *"the root of many of the logical problems with the ordinalist position"* is the fact that it makes no distinction between *"a Likert response format, a Likert (graded valence) question (or stem) or a Likert scale (collection of items)."* Based on *"a wide array of additional supporting empirical evidence"*, the authors concluded that:

> *"It is, therefore, as the intervalists contend, perfectly appropriate to summarise the ratings generated from Likert scales using means and standard deviations, and it is perfectly appropriate to use parametric techniques like Analysis of Variance to analyse Likert scales"* [@Carifio:2008]. 

However, @Carifio:2008 also stated that analyzing individual Likert items *"is a practice that should only occur very rarely."* This statement was criticized by @Norman:2010, who argued that it does not convincingly refute the *"ordinalist"* position. According to the author, since ANOVA and other parametric statistics are robust enough to deal with Likert data, even with small sizes and unequal variances, there is *"no fear of coming to the wrong conclusion."* In the same line, @Harper:2015 came up with a number of recommendations about these issues, including the following two: (1) *"Individual rating items with numerical response formats at least five categories in length may generally be treated as continuous data;"* and (2) *"Consider nonparametric or categorical data analysis approaches for individual rating items with numerical response formats containing four or fewer categories."* However, @Liddell:2018 identified several situations in which standard parametric methods fail, even when applied to averages of multiple items in Likert scales. In particular, the authors demonstrated how they can deform interaction effects or inflate errors when assumptions of equal variances are not met. 

Our experimental results on individual Likert items with 5, 7, and 11 levels show that parametric ANOVA is robust for the analysis of main effects, as long as variances are equal. On the other hand, we observed that parametric ANOVA method often fails to correctly infer interactions when main effects become large, and the problem becomes more apparent when ordinal thresholds are not equidistant. INT appears to reduce the impact of this problem but without offering a satisfying solution. We also found that at least for 5-level items with flexible thresholds, all methods may lead to unprecise effect size estimates. In conclusion, while ANOVA seems to be a valid method for hypothesis testing even based on the analysis of individual Likert-item responses, we identified situations that require special attention. Unfortunately, nonparametric approaches may not necessarily work well in these situations. 

In our prior research [@fages:2022], we applied ordered probit models [@Burkner:2019;@Christensen2023ordinal] for analyzing ordinal responses in two experimental studies. Since the HCI community is not familiar with these models, we found it necessary to provide motivation for their use and offer additional explanations to reviewers and readers on how to interpret results. These models offer numerous advantages, including their ability to support interval estimation and prediction in addition to null hypothesis testing. However, researchers are required to acquaint themselves with concepts of generalized mixed-effects models or Bayesian inference in order to use them correctly. We hope that our examples in the paper and the supplementary materials can inspire the community to give these methods more thoughtful consideration. 
 
### Interpreting interactions
 We discussed how the scale of the data affects the interpretation of interactions [@Loftus:1978]. In particular, when distributional assumptions are violated, a positive test for interaction may not indicate a genuine interaction. We demonstrated that nonparametric methods do not eliminate this problem, as they also struggle to control the Type I error rate of interactions in the presence of strong parallel main effects. Larger sample sizes do not mitigate this issue; instead, the ratio of false positives increases with sample size because the test becomes more sensitive. 
  
A useful method for diagnosing such issues is to plot interactions (e.g., see @fig-removable-interactions) and visually inspect the trends of the effects. @Loftus:1978 notes that interactions that do not cross over are removable under monotonic transformations, and therefore they cannot be interpreted. In constrast, interactions that cross over are non-removable. @fig-removable-interactions presents examples of a removable and a non-removable interaction. It is noteworthy that the crossover of the non-removable interaction is only discernible on the second chart (in <span style="color:#FF5E00"> **orange-red**</span>), where each line shows the effect of Technique.

::: {#fig-removable-interactions}
```{r, echo=FALSE, message=FALSE, fig.height=3.5, fig.width = 9, warning=FALSE}
df1 <- data.frame(Difficulty = c("easy","hard","easy","hard"), Technique=c("Tech A","Tech A","Tech B","Tech B"),
   Time=c(0.3, 1.1, 1.5, 3.8))

df2 <- data.frame(Difficulty = c("easy","hard","easy","hard"), Technique=c("Tech A","Tech A","Tech B","Tech B"),
   Time=c(1.5, 0.4, 2.5, 3.8))

annotations = list( 
  list( 
    x = 0.2,  
    y = 1.0,  
    text = "removable interaction",  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ),  
  list( 
    x = 0.8,  
    y = 1,  
    text = "nonremovable interaction",  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ))

fig1 <- createInteractionPlot(df1, palette = c("#888888", "#888888"))
fig2 <- createInteractionPlotInv(df1, palette = c("#FF5E00", "#FF5E00"))
fig3 <- createInteractionPlot(df2, offx = -0.3, palette = c("#888888", "#888888"))
fig4 <- createInteractionPlotInv(df2, offx = -0.65, offy = 1.5, palette = c("#FF5E00", "#FF5E00"))

figA <- subplot(fig1, fig2, titleY = TRUE, shareY = TRUE, titleX = TRUE, margin = 0.04)
figB <- subplot(fig3, fig4, titleY = TRUE, shareY = TRUE, titleX = TRUE, margin = 0.04)

fig <- subplot(figA, figB, titleY = TRUE, margin = 0.06) %>%   
  layout(annotations = annotations) %>%
  config(displayModeBar = FALSE, scrollZoom = FALSE, displaylogo = FALSE) %>% layout(hovermode = 'x', dragmode='pan')

fig
```
Examples of a removable (left) and a non-removable interaction (right). A removable interaction does not cross over, regardless of how it is plotted. 
:::

### Research planning and transparency
Planning analysis methods before data collection is a sensible practice, as it helps focus on measures that are easy to analyze and communicate, and anticipate the types of responses they may generate. We also encourage authors to open their datasets so that other researchers can replicate their findings and evaluate the robustness of their analysis methods. Although ART has been used in a large number of studies, very few datasets from these studies are openly available. Therefore, the extent to which the use of this method has contaminated research outcomes remains unclear and unfortunately difficult to evaluate.