### Type I errors under unequal variances
Many statistical procedures assume equal variances across all levels of each independent variable, or more strictly, across all possible pairs of factor levels (the latter is known as the sphericity assumption in repeated-measures ANOVA). Nonparametric tests are often mistakenly assumed to be free of such assumptions; however, as discussed earlier, this is generally incorrect.

Our next experiment evaluates the behavior of the four methods under conditions of unequal variances, focusing on the normal distribution and four ordinal distributions. For ordinal outcomes, we control variances at the level of the latent variable $Y^*$. 

We examine three two-factor designs: (i) a $4 \times 3$ within-subject design; (ii) a $2 \times 3$ between-subject design; and (iii) a $2 \times 4$ mixed design. We set all effects to zero and vary the ratio $r _{sd}$ of standard deviations between the levels of the first factor $X_1$. @fig-unequal shows the five ratios tested when $X_1$ has two levels. When $X_1$ has additional levels (as in the $4 \times 3$ design), their standard deviations are randomly drawn from a range bounded by the standard deviations of the first two levels. 

::: {#fig-unequal}
```{r, echo=FALSE, message=FALSE, fig.height=1.5, fig.width = 9, warning=FALSE}
plot_unequal_variances()
```
Varying the ratio $r _{sd}$ of standard deviations across the levels of $X_1$. The plots show examples of population distributions for factors with two categorical levels.    
:::

**Main effects**. We begin by investigating how the four methods detect main effects of $X_1$. Interpreting results in this setting is challenging because the underlying populations differ in variance while sharing the same means (and medians). For ordinal scales, differences in variance may also manifest as differences in observed means [@Liddell:2018]. To highlight these interpretation issues, we therefore label the observed rates of positives as *Positives (%)*. These rates can nonetheless be interpreted as Type I error rates if the null hypothesis is defined in terms of mean differences on the latent variable.

@fig-unequal-main-1 presents the results. Under normal distributions, and depending on the experimental design, the different methods either slightly inflate or slightly deflate the positive rates as $r _{sd}$ increases, with no method clearly outperforming the others. For ordinal scales, larger values of $r _{sd}$ primarily affect conditions with flexible thresholds. This effect is particularly pronounced for PAR, although all methods are affected.

::: {#fig-unequal-main-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "Type_I_heteroscedacity"
distributions = c("norm", "likert_5", "likert_5_flex", "likert_7", "likert_7_flex")
dnames = c("Normal", "Equidistant (5 levels)", "Flexible (5 levels)", "Equidistant (7 levels)", "Flexible (7 levels)")

df <- read_data(prefix, alpha = 0.05, effectType = 1, distributions)
df <- reshape_by_design(df, dnames, effectvars = c("sdratio", "effectX1","effectX2","effectX1X2"))
plotly_error_by_design_2(df, xlab = "max ratio of standard deviations between levels of X1", ytitle = 'Positives (%)', var = "rateX1", xvar = "sdratio", max = 41)
```
Positive rates ($\alpha = .05$) for the **main effect of $X_1$** as the ratio of standard deviations $r _{sd}$ for $X_1$ increases ($n=20$). *Note: When $r _{sd} > 1$, the underlying populations are no longer identical, and different interpretations of the null hypothesis may lead to different outcomes.*
:::

We also examine the influence of unequal variances across the levels of $X_1$ on the Type I error rate for $X_2$, as shown in @fig-unequal-main-2. In this case, there is no ambiguity in the interpretation of effects because the populations defined by $X_2$ are identical.

The Type I error rates of PAR, RNK, and INT appear largely unaffected by increases in $r _{sd}$. In contrast, ART yields high error rates, surpassing acceptable levels even under normal distributions (see, for example, the $2 \times 4$ mixed design). Thus, unequal variances pose a more serious problem for ART than for the other methods, as the method's sensitivity propagates across factors. This provides further evidence that ART's alignment procedure leads to confounding across effects.

::: {#fig-unequal-main-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotly_error_by_design_2(df, xlab = "max ratio of standard deviations between levels of X1", var = "rateX2", xvar = "sdratio", max = 41)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as the ratio of standard deviations $r _{sd}$ across the levels of $X_1$ increases ($n=20$)
:::

**Interaction effects**. Finally, we evaluate Type I error rates for the interaction, as shown in @fig-unequal-interaction. ART exhibits a similar pattern as before, with error rates increasing as $r _{sd}$ grows. We also observe some inflation of Type I error rates for the other three methods under the $4 \times 3$ within-subject design, albeit to a significantly lesser degree.

::: {#fig-unequal-interaction}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotly_error_by_design_2(df, xlab = "max ratio of standard deviations between levels of X1", var = "rateX1X2", xvar = "sdratio", max = 42)
```
Type I error rates ($\alpha = .05$) for the **interaction effect $X_1 \times X_2$** as the ratio of standard deviations $r _{sd}$ across the levels of $X_1$ increases ($n=20$)
:::
