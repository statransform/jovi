---
title: "The illusory promise of the Aligned Rank Transform"
subtitle: "A systematic study of rank transformations"
author: 
  - name: Theophanis Tsandilas
    orcid: 0000-0002-0158-228X
    email: theophanis.tsandilas@inria.fr
    affiliations:
      - name: Université Paris-Saclay, CNRS, Inria, LISN
        country: France
  - name: Géry Casiez
    orcid: 0000-0003-1905-815X
    email: gery.casiezuniv-lille.fr
    affiliations:
      - name: Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL
        country: France
bibliography: bibliography.bib
csl: canadian-journal-of-philosophy.csl

tbl-cap-location: top
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Useful libraries
library(crosstalk)
library(kableExtra)
library(gridExtra)
library(lmerTest)
library(tidyverse)
library(plotly)
```

```{=html}
<style>
.math.inline .MathJax  {
  font-size: 105% !important;
}
</style>
```


## Experimental method {#methodology}
We can now detail our experimental method. We evaluate the standard parametric approach (*PAR*) and the three rank-transformation methods (*RNK*, *INT*, and *ART*) that we introduced earlier. We conduct a series of Monte Carlo experiments that assess their performance under a variety of experimental configurations:

**Distributions.** We evaluate both *ratio* and *ordinal* data, drawn from the following distributions: 

1. *Normal distribution* $\mathcal{N}(\mu, \sigma)$, where $\mu$ is its mean and $\sigma^2$ is its standard deviation. For most experiments where variances are equal, we set $\sigma = 1$.
2. *Log-normal distribution* $\mathcal{LogN}(\mu, \sigma)$, where $\mu$ is its mean and $\sigma^2$ is its standard deviation at the logarithmic scale. The log-normal distribution is a good model for various measures bounded by zero, such as task-completion times. We set $\sigma = 1$ in our main experiments but evaluate a wider range of parameters in [Appendix I](appendix_I.html#lognormal).
3. *Exponential distribution* $Exp(\lambda)$, where $\lambda$ is its rate. The exponential distribution naturally emerges when describing the time elapsed between events. For example, we could use it to model the time a random person spends with a public display, or the waiting time before a new person approaches to interact with the display, when the average waiting time is $\frac{1}{\lambda}$.  
4. *Cauchy distribution* $\mathcal{Cauchy}(x_0, \gamma)$, where $x_0$ defines its location (median) and $\gamma$ defines its scale. The Cauchy distribution is the distribution of the ratio of two independent normally distributed random variables. It rarely emerges in practice. However, it is commonly used in statistics to test the robustness of statistical procedures because both its mean and variance are undefined. As we discussed earlier, past evaluations of ART [@Mansouri:1995; @elkin:2021] show that the method fails under the Cauchy distribution. We set $\gamma = 1$.
5. *Poisson distribution* $\mathcal{Pois}(\lambda)$, where $\lambda$ is its rate. It expresses the probability of a given number of events in a fixed interval of time. For example, we could use it to model the number of people who interact with a public display in an hour, when the average rate is $\lambda$ people per hour.
6. *Binomial distribution* $\mathcal{B}(\kappa, p)$, where $\kappa$ defines the number of independent Bernoulli trials and $p$ is the probability of error or success of each trial. It frequently appears in HCI research, as it can model the number of successes and failures in a series of experimental tasks. We set $\kappa = 10$ in our main experiments but evaluate a wider range of parameters in [Appendix I](appendix_I.html#binomial).
7. Distributions of *Likert-item* responses with 5, 7, and 11 levels.

**Experimental designs.** We present results for five experimental designs. To simplify our presentation, we start with a 4 $\times$ 3 within-subjects factorial design. We then show how our conclusions generalize to four additional designs: (i) a 2 $\times$ 3 between-subjects design; (ii) a 2 $\times$ 4 mixed design, with a between-subjects factor and a within-subjects factor; (iii) a 2 $\times$ 2 $\times$ 2 within-subjects design, and (iv) a 3 $\times$ 3 $\times$ 3 within-subjects design.

**Sample sizes.** We focus on three sample sizes, $n=10$, $n=20$, and $n=30$, where $n$ represents the cell size in an experimental design. However, for certain scenarios, we also report results for larger sample sizes, up to $n = 512$. In within-subjects designs, where all factors are treated as repeated measures, $n$ corresponds to the number of subjects $N$ (commonly human participants in HCI research). In contrast, in a 2 $\times$ 3 between-subjects design, a cell size of $n = 20$, implies a total of $N = 120$ subjects. 

**Equal vs. unequal variances.** We test the robustness of the methods when the variances on the latent variable are unequal. 

**Evaluation measures.** In addition to Type I error rates, we compare the statistical power of the methods and assess the accuracy of their effect size estimates.

**Magnitude of effects.** We analyze both main and interaction effects, examining how increasing the magnitude of effect of one factor influences the Type I error rates of other factors and their interactions. In addition, we examine how increasing two main effects in parallel influences the Type I error rate of their interaction, while emphasizing that the definition of the null hypothesis in this case is ambiguous.    

<!--
Furthermore, we assess how a growing interaction effect, either in isolation or in combination with a main effect, can affect the inference of main effects. -->

Previous evaluations of rank transformation methods [@Beasley:2009; @luepsen:2018] have also examined unbalanced designs, where cell sizes vary across the levels of a factor. When combined with unequal variances, such designs often pose challenges for both parametric procedures [@blanca:2018] and rank transformation methods [@Beasley:2009; @luepsen:2018]. As noted earlier, we do not consider unbalanced designs in our main article. However, we provide additional experimental results on missing data in [Appendix I](appendix_I.html#missing).

### Statistical modeling
For simplicity, we explain here our modeling approach for two factors, but its extension to three factors is straightforward. 

**Linear component.** The base of our data generation process for all distributions is the following linear term:

$$ 
\eta_{ijk} = \mu + s_k + a_1 x_{1i} + a_2 x_{2j} + a_{12} x_{1i} x_{2j}
$${#eq-linear-model}

 - $\mu$ is the grand mean

 - $s_k \sim \mathcal{N}(0, \sigma_s^2)$ is the random intercept effect of the *k*-th subject, where $k = 1..n$ 

 - $x_{1i}$ is a numerical encoding of the *i*-th level of factor $X_1$, where $i = 1..m_1$

 - $x_{2j}$ is a numerical encoding of the *j*-th level of factor $X_2$, where $j = 1..m_2$

 - $a_1$, $a_2$, and $a_{12}$ express the magnitude of main and interaction effects 

<!--
 - $\varepsilon_{ijk}$ is the experimental error effect
 -->

 While random slope effects can have an impact on real experimental data [@barr:2013], we do not consider them here for two main reasons: (1) to be consistent with previous evaluations of the ART procedure [@elkin:2021]; and (2) because mixed-effects procedures with random slope effects are computationally demanding, adding strain to simulation resources. However, there is no good reason to believe that adding random slope effects would impact our findings and conclusions.

**Encoding the levels of factors.** To encode the levels of the two factors $x_{1i} \in X_1$ and $x_{2j} \in X_2$ we proceed as follows: 

1. We normalize the distance between their first and their second levels such that $x_{12} - x_{11} = 1$ and $x_{22} - x_{21} = 1$. This approach enables us to conveniently control for the main and interaction effects by simply varying the parameters $a_1$, $a_2$, and $a_{12}$.

2. For the remaining levels, we randomly sample from a uniform distribution that spans the range between these two extreme levels, i.e., between $x_{11}$ and $x_{12}$ for $X_1$, and between $x_{21}$ and $x_{22}$ for $X_2$. This approach allows us to generate and evaluate a substantial variety of configurations, each representing different relative effects between levels.  

3. We require all levels to sum up to 0, or $\sum\limits_{i=1}^{m_1}  x_{1i} = 0$ and $\sum\limits_{j=1}^{m_2}  x_{2j} = 0$, which ensures that the grand mean is $\mu$.

For example, we can encode a factor with four levels as $\{-.6, .4, .1, .1\}$ or as $\{-.5, .5, .3, -.3\}$. 

**Generating ratio data.** We adopt generalized linear modeling for ratio data, where we use an inverse link function $g^{-1}$ to make the connection between $\eta_{ijk}$ and the location parameter of the distribution of interest:  
$$
Y_{ijk} \sim \mathcal{F}(location=g^{-1}(\eta_{ijk}), ...)
$${#eq-glm-ratio}

where $\mathcal{F}$ denotes the probability density (or mass) function from which we draw samples. For example, the mean of the exponential distribution is equal to $1 / \lambda$, where $\lambda$ is its rate parameter. This mean is linked to the linear term through the function $g^{-1}(x) = e^{x}$, and therefore, $\lambda = e^{-\eta_{ijk}}$.

 @tbl-distributions presents how the parameters of the other distributions are linked to the linear term.

| distribution | parameters | description               |
|--------------|------------|---------------------------|
| Normal       |$\mu = \eta_{ijk}$ <br> $\sigma^2 = \sigma_{\varepsilon}^2$ | mean <br> standard deviation |
| Log-normal    |$\mu = \eta_{ijk}$ <br> $\sigma^2 = \sigma_{\varepsilon}^2$ | mean at logarithmic scale (logarithm of median) <br> standard deviation at logarithmic scale|
| Exponential  |$\lambda = e^{-\eta_{ijk}}$ | rate (1 / mean) |
| Cauchy       |$x_0 = \eta_{ijk}$ <br> $\gamma = 1$ | location (median) <br> scale 
| Poisson      |$\lambda = e^{\eta_{ijk}}$ | rate (mean) |
| Binomial     |$p = Logistic(\eta_{ijk})$ <br> $\kappa = 10$ | probability of error <br> number of Bernoulli trials |

: Distribution parameters and their links to the linear term $\eta_{ijk}$ {#tbl-distributions}

**Generating ordinal data.**


### Population control and distribution conversions 
To simplify our simulations, we fix the following population parameters: $\mu = 0$, $\sigma = 1$, and $\sigma_s = 1$. We then control the magnitude of effects by varying $a_1$, $a_2$, and, for some experiments $a_{12}$. @fig-effects presents the range of values that we test for $a_1$ and $a_2$. We also visualize their effects on the latent variable for factors with two, three, and four categorical levels. 

::: {#fig-effects}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 9, warning=FALSE}
source("effects_plot.R")
plotEffects()
```
Varying $a_1$ or $a_2$ to control the magnitude of main effects. The plots show examples of population distributions on the latent variable $\mathcal{Y}$ for factors ($X_1$ or $X_2$) with two (top), three (middle), or four categorical levels (bottom).    
:::

We follow the approach of @DeBruine:2021 and use the R package *faux* v1.2.1 [@faux] to simulate data for our mixed-effects models. We assume that the distributions of random intercepts and errors are normal, or $s_k \sim N(0,\sigma_s)$ and $\varepsilon_{ijk} \sim N(0,\sigma)$.
To simulate the observed variable $Y$, we then transform the response values $y_{ijk}$ as described in [Section 3](#approach). A key advantage of this method is that we can generate responses for any distribution, while we control effects on the latent variable in the exact same way.

### Implementation of rank transformation methods
For the aligned rank transformation (ART), we use the R implementation of ARTool v0.11.1 [@artool]. For the pure rank transformation (RNK), we use R's *rank()* function. We use the *Rankit* formulation [@Bliss:1956] for the inverse normal transformation (INT), as explained earlier. Unless explicitly mentioned otherwise, we use the formula ``Y' ~ X1*X2 + (1|S)`` with R's *lmer* function, except for the between-subjects design where we use the formula ``Y' ~ X1*X2 + Error(S)`` with R's *aov* function.

### Evaluation measures
Significance tests have two types of errors. *Type I errors*, or false positives, are mistaken rejections of the null hypothesis. Type II errors, of false negatives, are failures to reject a null hypothesis that is actually true. In our illustrative example in @fig-example, a Type I error is finding that there is an effect of the choice of the technique on time performance. A *Type II error*, in turn, is finding that the task difficulty has no effect on time performance. 

Statistical significance testing requires setting a significance threshold known as significance or $\alpha$ (alpha) level, with typical values $\alpha = .05$ and $\alpha = .01$. The Type I error rate of a well-behaved significance test should be close to this nominal alpha level. An error rate clearly above this level suggests that the significance test is too liberal, while an error rate clearly below this level suggests that the test is too conservative. Four of our experiments specifically assess the Type I error rate of the methods. We test two significance levels: $\alpha = .05$ and $\alpha = .01$. For brevity, we only report results for $\alpha = .05$ in the main paper and include additional results in our supplementary material. 

We do not directly evaluate Type II errors. Instead, we report on statistical *power* defined as $Power = 1 - \beta$, where $\beta$ is the rate of Type II errors. Significance tests do not provide any power guarantees. However, we can compare the power of different methods to evaluate their relative performance. In addition to power, we assess effect size estimates, where we use as ground truth the estimates of a parametric ANOVA conducted on the latent variable $\mathcal{Y}$. While partial $\eta^2$ is the most commonly used effect size measure, we also evaluate Cohen's $f$, as its expected value is proportional to the real magnitude of effect. However, $\eta^2$ can be directly derived from Cohen's $f$ as follows:

$$ 
\eta^2 = \frac{f^2}{1 + f^2}
$${#eq-cohensf}

As explained earlier, interaction effects are distorted when the latent variable $\mathcal{Y}$ is transformed to produce the observed responses. These transformations also influence the Type I error rates and statistical power we observe. In this analysis, we focus on evaluating these measures with respect to the effects applied to the latent variable $\mathcal{Y}$. We explicitly make this distinction whenever it is relevant to our discussion. 

### Hardware platform and iterations
Our experimental R code is available in our supplementary material. We ran our experiments separately in a cluster of 8 machines Dell R640 Intel Xeon Silver 4112 2.6GHz with 4 cores and 64 GB memory. Our R code was parallelized to use all four cores of each machine. Some experiments took a few hours to complete, while others took several days. 

To estimate the power and Type I error rates of the four methods with enough precision, we ran $5000$ iterations for each population configuration and each sample size. 