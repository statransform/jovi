---
title: "The illusory promise of the Aligned Rank Transform"
subtitle: "Appendix I. Additional experimental results"
author: 
  - name: Theophanis Tsandilas
    orcid: 0000-0002-0158-228X
    email: theophanis.tsandilas@inria.fr
    affiliations:
      - name: Université Paris-Saclay, CNRS, Inria, LISN
        country: France
  - name: Géry Casiez
    orcid: 0000-0003-1905-815X
    email: gery.casiezuniv-lille.fr
    affiliations:
      - name: Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL
        country: France
bibliography: bibliography.bib
csl: canadian-journal-of-philosophy.csl

tbl-cap-location: top
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Useful libraries
library(crosstalk)
library(kableExtra)
library(gridExtra)
library(lmerTest)
library(tidyverse)
library(plotly)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Source code for reading data from experimental results and for plotting
source("dataReaders.R")
source("plotting.R")

library(dplyr)
library(tidyverse)

source("utils-data-reading.R")
source("utils-plotlying.R")

source("plotlying.R")
source("interactions_plot.R")
```

```{=html}
<style>
.math.inline .MathJax  {
  font-size: 105% !important;
}
</style>
```

We present complementary results and new experiments that investigate additional scenarios. We also compare INT and RNK with other nonparametric methods. Unless explicitly mentioned in each section, we follow the experimental methodology presented in the main article. At the end of each section, we summarize our conclusions.

## Results for $\alpha = .01$ {#alpha}
Although we only presented results for $\alpha = .05$ in the main article, we observe the same trends for other significance levels. The following figures show the Type I error rates of the methods for the $4 \times 3$ within-subjects design when $\alpha = .01$. Note that error rates are not proportional to the $\alpha$ level.
For results from other experiments, we refer readers to our raw result files.  

**Main effects**. @fig-ratio-main presents Type I error rates for the main effect of $X_2$ as the magnitude of the main effect of $X_1$ increases.

::: {#fig-ratio-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix1 <- "Type_I_4x3_ratio"
alpha <- .01
distributions1 <- c("norm", "lnorm", "exp", "poisson", "binom")
dnames1 <- c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial")
df1 <- readly_data(prefix1, alpha, 1, distributions1, dnames1)

prefix2 <- "Type_I_ordinal"
distributions2 <- c("likert_5_flex")
dnames2 <- c("Ordinal (5 levels)")
df2 <- readly_data(prefix2, alpha, 1, distributions2, dnames2)

df <- rbind(df1,df2)

plotly_error(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 103)
```
Type I error rates ($\alpha = .01$) for the **main effect of $X_2$** as a function of the magnitude $a_1$ of the main effect of $X_1$
:::

**Interaction effects**. @fig-ratio-interaction-1 presents Type I error rates for the interaction effect $X_1 \times X_2$, when the main effect on $X_2$ is zero while the main effect on $X_1$ increases.

::: {#fig-ratio-interaction-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotly_error(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 103)
```
Type I error rates ($\alpha = .01$) for the **interaction** $X_1 \times X_2$ as a function of the magnitude $a_1$ of the main effect of $X_1$ 
:::

**Interactions under parallel main effects**. We also present results for interaction effects when the two main effects change in parallel. As we explain in the main paper, these results require special attention, as whether positives rates are considered as Type I error rates depends on the way we define the null hypothesis for interactions.

::: {#fig-ratio-interaction-2}
```{r, echo=FALSE, message=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df1 <- readly_data(prefix1, alpha, 0, distributions1, dnames1)
df2 <- readly_data(prefix2, alpha, 0, distributions2, dnames2)
df <- rbind(df1,df2)

plotly_error(df, xlab = "magnitude of main effects", ytitle = 'Positives (%)',var = "rateX1X2", xvar = "effectX1", max = 104)
```
Positive rates ($\alpha = .01$) for the **interaction** $X_1 \times X_2$ as a function of the magnitudes $a_1 = a_2$ of the main effects of $X_1$ and $X_2$. *Note: These values correspond to Type I error rates under the null hypothesis $a_{12} = 0$; alternative definitions of the null hypothesis may lead to different results.* 
:::

### Conclusion
Results for $\alpha = .01$ are consistent with our results for $\alpha = .05$.


## Missing data {#missing}
We evaluate how missing data affect the performance of the four methods. Specifically, we consider scenarios in which a random sample of $10\%$, $20\%$, or $30\%$ of the observations is missing. Our analysis is restricted to normal distributions and includes two conditions: (i) equal variances and (ii) unequal variances across the levels of the factor to effects are applied ($X_1$ in our experiments), with the maximum ratio of standard deviations fixed to $r_{sd}=2$.

Missing observations lead to unbalanced designs, for which Type I hypothesis tests obtained using the `aov()` function are difficult to interpret. Accordingly, we use LMER and conduct Type III tests [@Kuznetsova:2017], focusing on a $4 \times 3$ within-subjects design and a $2 \times 4$ mixed design.  

We emphasize, however, that our scenarios do not include systematic imbalances arising from missing data concentrated in specific factor levels.  

**Main effects**. @fig-missing-main presents Type I error rates for the main effect of $X_2$ as the magnitude of the main effect of $X_1$ increases. We find that missing data cause ART's error rates to increase, with a more pronounced effects in the mixed design. This additional inflation compounds the error introduced by unequal variances. In contrast, the accuracy of the three other methods appears largely unaffected. 

::: {#fig-missing-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "Type_I_missing"
distributions = c("norm")

dnames <- c("10% - equal", "10% - unequal",  "20% - equal", "20% - unequal", "30% - equal", "30% - unequal")
df <- read_data(prefix, alpha = .05, effectType = 1, distributions)
df$family <- paste(df$missingratio*100, "% - ", ifelse(df$sdratio == 1, "equal", "unequal"), sep="") 
df <- reshape_by_design(df, dnames = dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

plotly_error_by_design_3(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 81)

```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude $a_{1}$ of the main effect of $X_1$, when $10\%$, $20\%$, or $30\%$ of the observations are missing ($n=20$).
:::

**Interaction effects**. We also examine Type I error rates for the interaction effect. In this case, ART's error rates are not influenced by missing data; the inflated errors observed in the mixed design are due to unequal variances.

::: {#fig-missing-interaction1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotly_error_by_design_3(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 21)

```
Type I error rates ($\alpha = .05$) for the **interaction effect $X_1 \times X_2$** as a function of the magnitude $a_{1}$ of the main effect of $X_1$, when $10\%$, $20\%$, or $30\%$ of the observations are missing ($n=20$).
:::

### Conclusion
ART is sensitive to the presence of missing data. Even under normal distributions, its Type I error rates for main effects increase. 



## Log-normal distributions {#log-normal}
We evaluate log-normal distributions across a wider range of $\sigma$ values (see @fig-lognormal-distributions), with particular attention to distributions with smaller variance, which exhibit lower degrees of skewness. 

::: {#fig-lognormal-distributions}
```{r, echo=FALSE, fig.height=1.3, fig.width = 9, warning=FALSE,  message=FALSE}
source("distributions_plots.R")
plot_effects_lognormal()
```
Log-normal distributions that we evaluate, shown here for a factor with two levels and a magnitude of effect $a_{1} = 2$. 
:::

**Main effects**. @fig-lognormal-main presents the Type I error rates for main effects. As expected, ART’s inflation of error rates is less severe when the distributions are closer to normal, whereas the problem becomes more pronounced as skewness increases.

::: {#fig-lognormal-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "Type_I_lognormal"
distributions = c("lnorm_0.2", "lnorm_0.4", "lnorm_0.6", "lnorm_0.8", "lnorm_1.0", "lnorm_1.2")
dnames = c("sd = 0.2", "sd = 0.4", "sd = 0.6", "sd = 0.8", "sd = 1.0", "sd = 1.2")

df <- read_data(prefix, alpha = .05, effectType = 1, distributions)
df <- reshape_by_design(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotly_error_by_design_2(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 103)

```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude $a_{1}$ of the main effect of $X_1$ ($n=20$)
:::

**Interaction effects**. We observe similar patterns for the Type I error rates of interaction effects in the presence of a single main effect. Notice that PAR's error rates are highly unstable. They appears inflated under the within-subjects design and deflated under other configurations. 

::: {#fig-lognormal-interaction1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotly_error_by_design_2(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 103)

```
Type I error rates ($\alpha = .05$) for the **interaction effect $X_1 \times X_2$** as a function of the magnitude $a_{1}$ of the main effect of $X_1$ ($n=20$)
:::

**Interactions under parallel main effects**. We also report results for scenarios in which both main effects increase in parallel. These results again require careful interpretation, as the null hypothesis of interest may differ across methods. Notably, even slight departures from normality (e.g., $\sigma = 0.2$) can make the interpretation of interaction effects problematic.  

::: {#fig-lognormal-interaction2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- read_data(prefix, alpha = .05, effectType = 0, distributions)
df <- reshape_by_design(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotly_error_by_design_2(df, xlab = "magnitude of main effects", ytitle = 'Positives (%)', var = "rateX1X2", xvar = "effectX1", max = 103)

```
Positive rates ($\alpha = .05$) for the **interaction effect $X_1 \times X_2$** as a function of the magnitudes $a_{1}$ and $a_{2}$ of the main effects ($n=20$). *Note: These values correspond to Type I error rates under the null hypothesis $a_{12} = 0$; alternative definitions of the null hypothesis may lead to different results.* 
:::

### Conclusion
ART’s robustness issues become less severe as log-normal distributions approach normality. However, even when variance is relatively small, Type I error rates can still become unacceptably large.


## Binomial distributions {#binomial}
We examine a broader range of parameter settings for the binomial distribution. Although we focus on the lower range of the success probability $p$, we expect analogous results for their symmetric values $1-p$. Specifically, we test $p=.05$, $.1$, and $.2$, and for each value we consider $\kappa=5$ and $10$ Bernoulli trials. 

**Main effects**. The results for main effects are presented in @fig-binom-main. For the within-subjects design, we find that ART’s Type I error rates increase as the number of trials decreases and as the probability of success approaches zero, reaching extremely high levels when $\kappa = 5$ and $p=.05$. However, trends change across designs. The other methods maintain good control of Type I error rates. 

::: {#fig-binom-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "Type_I_binomial"
distributions = c("binom_5_05", "binom_5_10", "binom_5_20", "binom_10_05", "binom_10_10", "binom_10_20")
dnames = c("5 trials, p=5%", "5 trials, p=10%", "5 trials, p=20%", "10 trials, p=5%", "10 trials, p=10%", "10 trials, p=20%")

df <- read_data(prefix, alpha = .05, effectType = 1, distributions)
df <- reshape_by_design(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotly_error_by_design_2(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 101)

```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude $a_{1}$ of the main effect of $X_1$ ($n=20$)
:::

**Interaction effects**. @fig-binom-interaction1 shows similar patterns for the Type I error rates of interaction effects in the presence of a single main effect. In the within-subjects design, however, PAR, RNK, and INT also tend to inflate error rates when $a_1 > 2$, although to a much smaller extent than ART.

::: {#fig-binom-interaction1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotly_error_by_design_2(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 101)

```
Type I error rates ($\alpha = .05$) for the **interaction effect $X_1 \times X_2$** as a function of the magnitude $a_{1}$ of the main effect of $X_1$ ($n=20$)
:::

**Interactions under parallel main effects**. When both main effects exceed a certain magnitude (see @fig-binom-interaction2), positive rates increase rapidilly for all methods. Whether this inflation is due to interpretation issues or to limitations in the robustness of the methods themselves, these results once again demonstrate the severe difficulties associated with testing interactions under such conditions.

::: {#fig-binom-interaction2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- read_data(prefix, alpha = .05, effectType = 0, distributions)
df <- reshape_by_design(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotly_error_by_design_2(df, xlab = "magnitude of main effects", ytitle = 'Positives (%)', var = "rateX1X2", xvar = "effectX1", max = 103)

```
Positive rates ($\alpha = .05$) for the **interaction effect $X_1 \times X_2$** as a function of the magnitudes $a_{1}$ and $a_{2}$ of the main effects ($n=20$). *Note: These values correspond to Type I error rates under the null hypothesis $a_{12} = 0$; alternative definitions of the null hypothesis may lead to different results.* 
:::

### Conclusion
ART performs extremely poorly for binomial data, producing very high Type I error rates even when all population effects are null. We also observe that testing interaction effects in the presence of parallel main effects is problematic for all methods.


## Main effects in the presence of interactions {#removable-main}
In all experiments assessing Type I error rates reported in our article, we assumed no interaction effects. However, we also need to understand whether weak or strong interaction effects could affect the sensitivity of the methods in detecting main effects. Unfortunately, the interpretation of main effects may become ambiguous when interactions are present. 

### Illustrative example 
To understand the problem, let us take a [dataset](removable_main.csv) from a fictional experiment (within-participants design with $n = 24$) that evaluates the performance of two techniques (*Tech A* and *Tech B*) under two task difficulty levels (*easy* vs. *hard*). @fig-example-interactions visualizes the means for each combination of the levels of the factors, using the original scale of measurements (left) and a logarithmic scale (right). Note that time measurements were drawn from log-normal distributions.

::: {#fig-example-interactions}
```{r, echo=FALSE, message=FALSE, fig.height=3.5, fig.width = 9, warning=FALSE}
df <- read.csv("removable_main.csv", sep=",", header=TRUE, strip.white=TRUE)
dftime <- aggregate(Time ~ Difficulty+Technique, data = df, mean)

fig1 <- createInteractionPlot(dftime) %>% layout( yaxis = list(hoverformat = '.2f'))
fig2 <- createInteractionPlot(dftime, logscale = TRUE) %>% layout( yaxis = list(hoverformat = '.2f'))

fig <- subplot(fig1, fig2, titleY = TRUE, titleX = TRUE, margin = 0.06) %>% layout(hovermode = 'x', dragmode='pan')

fig
```
The line charts visualize the effects of task difficulty (*easy* vs. *hard*) and technique (*Tech* A vs. *Tech B*) for task completion time. All data points represent group means. 
:::

We observe a clear main effect of *Difficulty* and a strong interaction effect, regardless of the scale. However, the main effect of *Technique* largely depends on the scale used to present the results. Can we then conclude that *Technique A* is overall faster than *Technique B*? 

Below, we present the results of the analysis using different methods.

|        | PAR  | LOG  | ART | RNK | INT |
|--------|------|------|-----|-----|-----|
| Difficulty  | $7.8 \times 10^{-8}$ | $4.3 \times 10^{-16}$  |  $8.6 \times 10^{-12}$ | $2.5 \times 10^{-16}$ | $1.1 \times 10^{-15}$ |
| Technique   | $.024$ | $.81$  | $.00017$ | $.69$ | $.69$ |
| Difficulty $\times$ Technique | $7.6 \times 10^{-6}$ | $4.1 \times 10^{-9}$ | $9.4 \times 10^{-8}$ | $1.1 \times 10^{-9}$ | $1.3 \times 10^{-9}$ |
: *p*-values for main and interaction effects {.sm}

PAR detects a main effect of *Technique* ($\alpha = .05$), as does ART, which yields an even smaller $p$-value. In contrast, LOG, RNK, and INT do not find no evidence for such an effect. As with *removable interactions*, we can speak about *removable main effects* in this case. 

As a general principle, it makes little sense to interpret main effects when crossing patterns emerge due to a strong interaction. In this scenario, the researchers should instead compare techniques within each level of *Difficulty*. Accordingly, we can conclude that *Technique A* is slower on *easy* tasks but faster on *hard* tasks in this experiment.

### Experiment
To better understand how the different methods detect main in the presence of interactions, we conduct a dedicated experiment. We focus on a $2 \times 2$ within-subjects experimental designs and examine perfectly symmetric cross-interactions. 

**Interaction effect only**. We first examine how the interaction effect alone influences the Type I error rate for $X_1$ (or $X_2$, since the design is fully symmetric). In this configuration, no interpretation issues arise: the true main effects are null under any reasonable definition of the null hypothesis.

The results are shown in @fig-interactions-main-1. We find that ART is the only method adversely affected under non-normal distributions, with Type I error rates increasing as the magnitude of the interaction effect grows.

::: {#fig-interactions-main-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
alpha <- .05

prefix <- "Type_I_non_null_interaction"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readly_data(prefix, alpha, effectType = 0, distributions, dnames)
plotly_error(df, xlab = "magnitude of interaction effect", var = "rateX1", xvar = "effectX1X2", max = 104)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_1$** as a function of the magnitude $a_{12}$ of the interaction effect $X_1 \times X_2$.
:::

**Interaction effect combined with main effect**. We evaluate the behavior of the methods for the effect of $X_2$ when a nonzero interaction effect is combined with a main effect on $X_1$. In this setting, the definition of the null hypothesis is inherently ambiguous, and the results must be interpreted with caution. We therefore report *Positives (%)* rather than Type I error rates. However, if the null hypothesis is defined as $a_2 = 0$, these values can be interpreted as Type I error rates.

The results are shown in @fig-interactions-main-3. None of the methods maintain positive rates at nominal levels. ART and PAR perform appropriately under normal distributions, but their rates increase dramatically for all non-normal distributions. In contrast, RNK and INT exhibit decreasing positive rates under continuous distributions, suggesting a loss of sensitivity to small main effects when strong main and interaction effects are present simultaneously. Under discrete distributions, however, both methods show substantial inflation of positive rates.

::: {#fig-interactions-main-3}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readly_data(prefix, alpha, effectType = 6, distributions, dnames)
plotly_error(df, xlab = "magnitude of interaction effect", ytitle = 'Positives (%)', var = "rateX2", xvar = "effectX1X2", max = 104)
```
Positive rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitudes $a_1 = a_{12}$ of a combined main effect on $X_1$ and an interaction effect $X_1 \times X_2$. *Note: These values correspond to Type I error rates under the null hypothesis $a_{2} = 0$; alternative definitions of the null hypothesis may lead to different results.* 
:::

### Conclusion
When strong interaction effects are present, the interpretation of main effects becomes especially problematic, particularly when those interactions co-occur with nonzero main effects. ART may even inflate Type I error rates in scenarios where such interpretation issues would not ordinarily be expected. More generally, these results underscore that main effects should be interpreted with great caution in the presence of strong interactions.

## ART with median alignment {#median-alignment}
We evaluate a modified implementation of ART (ART-MED), where we use medians instead of means to align ranks. This approach draws inspiration from results by @Salter:1993, showing that median alignment corrects ART's instable behavior under the Cauchy distribution. We only test the $4 \times 3$ within-participants design for sample sizes $n=10$, $20$, and $30$. For this experiment, we omit the RNK method and only present results for non-normal distributions. 

We emphasize that @Salter:1993 only apply mean and median alignment to interactions. Our implementation for main effects is based on the alignment approach of @wobbrock:2011, where we simply replace means by medians --- we are not aware of more adapted methods. 

**Main effects**. Our results presented in @fig-median-main demonstrate that median alignment (ART-MED) --- or at least our implementation of the method --- is not appropriate for testing main effects. Although Type I error rates are now lower for certain configurations (e.g., for the Cauchy distribution), they are higher in others. 

::: {#fig-median-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "Type_I_art_median"
alpha <- .05

methods = c("PAR", "INT", "ART", "ART-MED")
palette = c("#888888", "#009E73", "#FF5E00", "#FFA27F")
symbols = c("asterisk",  "star-diamond", "star-triangle-up", "star-triangle-down")

distributions <- c("lnorm", "exp", "cauchy", "poisson", "binom", "likert")
dnames <- c("Log-normal", "Exponential", "Cauchy", "Poisson", "Binomial", "Ordinal (5 levels)")
df <- readly_data(prefix, alpha, 1, distributions, dnames, methods = methods)

plotly_error(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 103, cbPalette = palette, symbols = symbols)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude $a_1$ of the main effect of $X_1$
:::

**Interaction effects**. In contrast, median alignment performs surprisingly well for interactions, correcting several deficiencies of ART, particularly when main effects are absent or weak. It appears to resolve ART's issues with discrete distributions and performs very well under the Cauchy distribution.  

Despite this improved performance, we do no recommend the method. It continues to struggle with skewed distributions, and its advantages over parametric ANOVA are essentially confined to the Cauchy distribution.

::: {#fig-median-interaction-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}

plotly_error(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 103, cbPalette = palette, symbols = symbols)
```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitude $a_1$ of the main effect of $X_1$ 
:::

**Interactions under parallel main effects**. We also examine the behavior of the methods for interactions in the presence of parallel main effects, where interpretation issues warrant particular caution. Once again, median alignment improves ART’s performance across all distributions, with its positive rates now approaching those of PAR.

::: {#fig-median-interaction-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readly_data(prefix, alpha, 0, distributions, dnames, methods = methods)
plotly_error(df, xlab = "magnitude of main effects", ytitle = 'Positives (%)', var = "rateX1X2", xvar = "effectX1", max = 105, cbPalette = palette, symbols = symbols)

```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitudes $a_1 = a_2$ of the main effects of $X_1$ and $X_2$. *Note: These values correspond to Type I error rates under the null hypothesis $a_{12} = 0$; alternative definitions of the null hypothesis may lead to different results.* 
:::

### Conclusion
Using median rather than mean alignment in ART substantially improves the method's performance for testing interactions across all distributions we examined. Nevertheless, we cannot recommend the approach. ART remains less robust than other rank-based methods, and its advantages over PAR are unclear. Moreover, it is not evident how median alignment should be applied to the testing of main effects --- simply using medians within the alignment procedure of @wobbrock:2011 leads to extremely high error rates.


## Nonparametric tests in single-factor designs {#nonparametric-tests}
We compare PAR, RNK, and INT to nonparamatric tests for within- and between-subjects single-factor designs, where the factor has two, three, or four levels. Depending on the design, we use different nonparametric tests. For within-subjects designs, we use the Wilcoxon sign-rank test if the factor has two levels (*2 within*) and the Friedman test if the factor has three (*3 within*) or four (*4 within*) levels. For between-subjects designs (*2 between*, *3 between*, and *4 between*), we use the Kruskal–Wallis test.

**Power**. @fig-nonparametric-power compares the power of the various methods as the magnitude of the main effect increases, where we use the abbreviation *NON* to designate a nonparametric test. We observe that primarily INT, but also RNK, generally exhibit better power than the nonparametric tests. Differences are more pronounced for within-subjects designs, corroborating Conover's [-@Conover:2012] observation that the rank transformation results in a test that is superior to the Friedman test under certain conditions. 

::: {#fig-nonparametric-power}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
source("plotlying-v3.R")

readSingle <- function(prefix, n, alpha, distributions, methods, POWER = FALSE) {
  	df <- read.csv(paste("data/", prefix, "_", n, ".csv" , sep=""), sep=",", header=TRUE, strip.white=TRUE)
	  df$distr <- factor(df$distr, levels=distributions)
	  df$method <- factor(df$method, levels=methods)

    if(POWER) {
      df <- df[df$effect > 0,]
    }
    else {
      df <- df[df$effect == 0,]
    }

    if(is.na(alpha)) return(df)
    else return(df[df$alpha == alpha,])
}

prefix <- "Appendix_test-One-Factor"
alpha = 0.05

distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")
methods = c("PAR", "RNK", "INT", "NON")
palette = c("#888888", "#E69F00", "#009E73", "#FF70AB")

df <- readSingle(prefix, n = 20, alpha, distributions, methods = methods, POWER = TRUE)
#df <- reshapeByDesign(df, dnames, effectvars = c("effect"))
df <- df %>% arrange(design,distr,effect,rates)  %>% group_by(design,distr,effect) %>% mutate(rank = rank(rates))

df <- as.data.frame(df) %>% reshapeByDesign(dnames, effectvars = c("effect"))
plotlyPowerByDesign_v3(df, xlab = "magnitude of effect", var = "rank", xvar = "effect", max = 4.3, cbPalette = palette)
```
Ranking of methods comparing their power ($\alpha = .05$) as a function of the magnitude of effect ($n = 20$). 
:::

We expect that the accuracy of ANOVA on rank-transformed values will decrease with smaller samples. However, our tests with smaller samples of $n=10$ show that INT remains robust and still outperforms other nonparametric methods. Although it is possibe to couple INT with permutation testing for higher accuracy [@Beasley:2009], we have not explored this possibility here.

**Type I error rate under equal and unequal variances**. @fig-nonparametric-unequal-var presents the rate of positives under conditions of equal ($r_{sd} = 0$) and unequal variances ($r_{sd} > 0$). While this rate can be considered a Type I error rate when variances are equal, interpreting it under other conditions requires special attention because the hypothesis of interest may differ among methods. Parametric ANOVA is particularly sensitive to unequal variances when distributions are skewed because it tests differences among means. While the normal distributions of the latent space have the same means, this is not the case with the skewed distributions of the transformed variable, which have the same median but different means. All nonparametric methods we tested use ranks, which preserve medians and mitigate this problem. However, their rate of positives can still exceed $5\%$ under certain conditions.

For between-subjects designs, we observe that the Kruskal–Wallis test and RNK yield very similar results. This is not surprising, as RNK is known to be a good approximation of the Kruskal–Wallis test [@Conover:2012]. INT's positive rates are similar, although slightly higher under the binomial distribution. For within-subjects designs, differences among methods are more pronounced. The Wilcoxon sign-rank test (*2 within*) inflates rates well above $5\%$, demonstrating that the test is not a pure test of medians. In contrast, the Friedman test (*3 within* and *4 within*) provides the best control among all methods. 

::: {#fig-nonparametric-unequal-var}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "Appendix_test-One-Factor-Unequal-Var"

df <- readSingle(prefix, n = 20, alpha, distributions, methods = methods)
df <- reshapeByDesign(df, dnames, effectvars = c("sd_ratio", "effect"))

plotlyErrorByDesign_v3(df, xlab = "max ratio of standard deviations", var = "rates", xvar = "sd_ratio", ytitle = 'Positives (%)', max = 24.2, nticks=8, cbPalette = palette)
```
Positives rates ($\alpha = .05$) as a function of the maximum ratio $r_{sd}$ of standard deviations between levels ($n = 20$). 
:::

@fig-nonparametric-unequal-var-alpha01 presents the same results but for $\alpha = .01$. Discrepancies among different methods are now more pronounced, and we notice again that the Friedman test keeps rates closer to the nominal level of $1\%$ compared to INT and RNK. Nevertheless, in addition to their greater power compared to the Friedman test, RNK or INT present other advantages, such as the possibility to use common ANOVA-based procedures to partly correct issues associated with unequal variances. For instance, we ran an experiment, where we applied a Greenhouse–Geisser correction following sphericity violations detected with the Mauchly's sphericity test ($\alpha = .05$). We found that this correction brings the error rates of INT close to nominal levels for continuous distributions, such as the normal and log-normal distributions. In the case of the binomial and ordinal distributions, error rates are also significantly reduced, well below those of the Friedman test, although not reaching nominal levels. 

::: {#fig-nonparametric-unequal-var-alpha01}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readSingle(prefix, n = 20, alpha= 0.01, distributions, methods = methods)
df <- reshapeByDesign(df, dnames, effectvars = c("sd_ratio", "effect"))

plotlyErrorByDesign_v3(df, xlab = "max ratio of standard deviations", var = "rates", xvar = "sd_ratio", ytitle = 'Positives (%)', max = 7.2, nticks=8, cbPalette = palette)
```
Positives rates ($\alpha = .01$) as a function of the maximum ratio $r_{sd}$ of standard deviations between levels ($n = 20$). 
:::

### Conclusion
We do not see significant benefits in using dedicated nonparametric tests over RNK or INT. INT can replace nonparametric tests even for single-factor designs. If, after transforming the data, the assumptions of homoscedasticity or sphericity are still not met, applying common correction procedures (e.g., a Greenhouse–Geisser correction for sphericity violations) on the transformed data can reduce the risk of Type I errors.

## ANOVA-type statistic (ATS) {#ATS}
We compare PAR, RNK, and INT to the ANOVA-type statistic (ATS) [@Brunner_ATS:2001] for two-factor designs. We use its implementation in the R package *nparLD* [@nparLD], which does not support between-subjects designs. Thus, we only evaluate it for the $4 \times 3$ within-subjects and the $2 \times 4$ mixed designs. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
source("plotlying-v2.R")
```

**Type I error rates: Main effects**. @fig-ATS-designs-main presents Type I error rates for the main effect of $X_2$. Under the mixed design, RNK, INT, and ATS exhibit very similar error rates, which are close to nominal levels. In the within-subjects design, the error rates of ATS tend to be slightly above $5\%$. Additionally, unlike the other methods whose error rates drop significantly below $5\%$ when the effect of $X_1$ becomes stronger under binomial and ordinal scales, the power of ATS does not seem to be affected in these cases. 

::: {#fig-ATS-designs-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "Appendix_test-ATS"
alpha = 0.05

distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")
methods = c("PAR", "RNK", "INT", "ATS")
palette = c("#888888", "#E69F00", "#009E73", "#FF70AB")

df <- readData(prefix, n = 20, alpha, effectType = 1, distributions, methods = methods)
df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

plotlyErrorByDesign_v2(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 7.1, nticks=8, cbPalette = palette)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude $a_1$ of the main effect of $X_1$ ($n = 20$)
:::

@fig-ATS-designs-main-2 presents results for the main effect of $X_1$. The error rates of ATS are now slightly inflated under the mixed design. The other methods exhibit the same trends as for the other factor. 

::: {#fig-ATS-designs-main-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df2 <- readData(prefix, n = 20, alpha, effectType = 2, distributions, methods = methods)
df2 <- reshapeByDesign(df2, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

plotlyErrorByDesign_v2(df2, xlab = "magnitude of main effect", var = "rateX1", xvar = "effectX2", max = 7.1, nticks=8, cbPalette = palette)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_1$** as a function of the magnitude $a_2$ of the main effect of $X_2$ ($n = 20$)
:::

**Type I error rates: Interactions**. @fig-ATS-designs-interaction-1 presents Type I error rates for the interaction in the presence of a single main effect. Results are again very similar for all three nonparametric methods under the mixed design. In contrast, the error rates of ATS tend to be lower than nominal levels under the within-subjects design, often falling below $4\%$.

::: {#fig-ATS-designs-interaction-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotlyErrorByDesign_v2(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 7.1, nticks=8, cbPalette = palette)
```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitude $a_1$ of the main effect of $X_1$ ($n = 20$) 
:::

 When two parallel main effects are present, ATS and RNK lead to very similar trends (see @fig-ATS-designs-interaction-2). Overall, INT appears to be a more robust method with the exception of the binomial distribution, for which error rates are higher for this method.  

::: {#fig-ATS-designs-interaction-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha, effectType = 0, distributions, methods = methods)
df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign_v2(df, xlab = "magnitude of main effects", var = "rateX1X2", xvar = "effectX1", max = 104, nticks=8, cbPalette = palette)
```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitudes $a_1 = a_2$ of the main effects of $X_1$ and $X_2$  ($n = 20$)
:::

**Power: Main effects**. As shown in @fig-ATS-power-main-1 and @fig-ATS-power-main-2, ATS appears as the most powerful method for detecting effects of $X_1$ for the mixed design. However, in all other situations, it has less power than INT and offer less power than RNK.  

::: {#fig-ATS-power-main-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
alpha <- .05

prefix <- "Appendix_test-ATS-Power"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha, effectType = 3, distributions, methods = methods)

df <- df %>% arrange(design,distr,effectX1,rateX1)  %>% group_by(design,distr,effectX1) %>% mutate(rank = rank(rateX1))
df <- as.data.frame(df) %>% reshapeByDesign(dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

#df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyPowerByDesign_v2(df, xlab = "magnitude of main effect", var = "rank", hovervar = "rateX1", xvar = "effectX1", max = 4.2, ytitle = 'Power (%) - ranking', cbPalette = palette)
```
Ranking of methods for their power ($\alpha = .05$) to detect the **main effect of $X_1$** as a function of the magnitude of effect $a_1$ ($n=20$). Hover over the charts to inspect their power ($\%$).
:::

::: {#fig-ATS-power-main-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readData(prefix, n = 20, alpha, effectType = 4, distributions, methods = methods) 

df <- df %>% arrange(design,distr,effectX2,rateX2)  %>% group_by(design,distr,effectX2) %>% mutate(rank = rank(rateX2))
df <- as.data.frame(df) %>% reshapeByDesign(dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

#df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyPowerByDesign_v2(df, xlab = "magnitude of main effect", var = "rank", hovervar = "rateX2", xvar = "effectX2", max = 4.2, ytitle = 'Power (%) - ranking', cbPalette = palette)
```
Ranking of methods for their power ($\alpha = .05$) to detect the **main effect of $X_2$** as a function of the magnitude of effect $a_2$ ($n=20$). Hover over the charts to inspect their power ($\%$).
:::

**Power: Interactions**. @fig-ATS-power-interaction shows results on power for interactions. INT emerges again as the most powerful method. The power of ATS is particularly low under the within-subjects design. 

::: {#fig-ATS-power-interaction}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readData(prefix, n = 20, alpha, effectType = 5, distributions, methods = methods)

df <- df %>% arrange(design,distr,effectX1X2,rateX1X2)  %>% group_by(design,distr,effectX1X2) %>% mutate(rank = rank(rateX1X2))
df <- as.data.frame(df) %>% reshapeByDesign(dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

#df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyPowerByDesign_v2(df, xlab = "magnitude of interaction effect", var = "rank", hovervar = "rateX1X2", xvar = "effectX1X2", max = 4.2, ytitle = 'Power (%) - ranking', cbPalette = palette)
```
Ranking of methods for their power ($\alpha = .05$) to detect the **interaction effect $X_1 \times X_2$** as a function of the magnitude of effect $a_{12}$ ($n=20$). Hover over the charts to inspect their power ($\%$).
:::

### Conclusion
Although ATS appears to be a valid alternative, it does not offer clear performance advantages over INT, which is also simpler and more versatile.

## Generalizations of nonparametric tests {#generalized}

Finally, we evaluate the generalizations of nonparametric tests recommended by Lüpsen [-@luepsen:2018; -@luepsen:2023] as implemented in his `np.anova` function [@luepsen_R]. Specifically, we examine the generalization of the van der Waerden test (VDW) and the generalization of the Kruskal-Wallis and Friedman tests (KWF). 

These implementations require random slopes to be included in the error term of the model. Concretely, we use ``Error(Subject/(X1*X2))`` for the two-factor within-participants design and ``Error(Subject/X2)`` for the mixed design. We use the same modeling approach for all other methods to ensure comparability.

**Type I error rates: Main effects**. @fig-vdWaerden-designs-main shows the Type I error rates for the main effect of $X_2$. While all methods perform well under the within-subjects and mixed designs, the error rates of VDW and KWF drop sharply as the effect of $X_1$ increases under the between-subjects design. As shown below, this behavior reflects a severe loss of power for these two methods in these conditions. We omit results for the other factor ($X_1$ as $a_2$ increases) as we observe very similar trends.

::: {#fig-vdWaerden-designs-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "Type_I_vdWaerden"
alpha = 0.05

distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")
methods = c("RNK", "INT", "VDW", "KWF")
palette = c("#E69F00", "#009E73", "#FF70AB", "#888888")

df <- read_data(prefix, alpha, effectType = 1, distributions, methods = methods)
df <- reshape_by_design(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

plotly_error_by_design_2(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 8.1, nticks=8, cbPalette = palette)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude $a_1$ of the main effect of $X_1$ ($n = 20$)
:::

**Type I error rates: Interactions**. @fig-vdWaerden-designs-interaction-1 presents the Type I error rates for the interaction in the presence of a single main effect. Again, the error rates of VDW and KWF decrease rapidly, now in both the between-subjects and mixed designs. We also observe inflation of the error rates of RNK and INT for large effects under discrete distributions.

::: {#fig-vdWaerden-designs-interaction-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- read_data(prefix, alpha, effectType = 2, distributions, methods = methods)
df <- reshape_by_design(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

plotly_error_by_design_2(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX2", max = 8.1, nticks=8, cbPalette = palette)
```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitude $a_2$ of the main effect of $X_2$ ($n = 20$) 
:::

@fig-vdWaerden-designs-interaction-2 shows the results when both $a_1$ and $a_2$ increase. In these settings, all four methods struggle as effect magnitudes grow, though their behavior varies across distributions and experimental designs. Depending on the configuration, the methods either inflate positive rates or deflate them below nominal levels. INT appears to be the most stable method for continuous distributions, but its positive rates increase more rapidly under discrete distributions.    

::: {#fig-vdWaerden-designs-interaction-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- read_data(prefix, alpha, effectType = 0, distributions, methods = methods)
df <- reshape_by_design(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotly_error_by_design_2(df, xlab = "magnitude of main effects", ytitle = 'Positives (%)', var = "rateX1X2", xvar = "effectX1", max = 104, nticks=8, cbPalette = palette)
```
Positive rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitudes $a_1 = a_2$ of the main effects of $X_1$ and $X_2$ ($n = 20$). *Note: These values correspond to Type I error rates under the null hypothesis $a_{12} = 0$; alternative definitions of the null hypothesis may lead to different results.* 
:::

**Power: Main effects**. We examine how power is affected by increasing the magnitude of the effect on the second factor. As shown in @fig-vdWaerden-power-main-1B, the power of all methods decreases, but KWF and VDW appear especially problematic under the between-subjects design (across all distributions) and the mixed design (for the log-normal and exponential distributions). INT once again emerges as the best-performing method, with the exception of the $2 \times 4$ mixed design under the Poisson and Binomial distributions, for which VDW and KWF exhibit a better behavior.

::: {#fig-vdWaerden-power-main-1B}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
alpha <- .05
prefix <- "Power_vdWaerden_multieffect"

df <- read_data(prefix, alpha, effectType = 3, distributions, methods = methods)
df <- reshape_by_design(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

plotly_error_by_design_2(df, xlab = "magnitude of main effect", var = "rateX1", xvar = "effectX2", ytitle ="Power (%)", max = 104, nticks=6, cbPalette = palette)
```
Power ($\alpha = .05$) for detecting the **main effect of $X_1$** ($a_1 = 0.8$) as a function of the magnitude of effect $a_2$ of the main effect of $X_2$ ($n=20$).
:::

**Power: Interactions**. Figure @fig-vdWaerden-power-interactionB shows how the power for detecting the interation effect is affected by the presence of a main effect. We observe that the power of all methods decreases as the main effect of $X_2$ increases, but this decline is substantially more pronounced for KWF and VDW, particularly in the between-subjects and mixed designs.

::: {#fig-vdWaerden-power-interactionB}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
alpha <- .05
prefix <- "Power_vdWaerden_multieffect"

df <- read_data(prefix, alpha, effectType = 5, distributions, methods = methods)
df <- reshape_by_design(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

plotly_error_by_design_2(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX2", ytitle ="Power (%)", max = 104, nticks=6, cbPalette = palette)
```
Power ($\alpha = .05$) for detecting the **interaction effect** ($a_{12} = 1.5$) as a function of the magnitude of effect $a_2$ of the main effect of $X_2$ ($n=20$).
:::

### Conclusion
Our results do not support the conclusions of Lüpsen [-@luepsen:2018; -@luepsen:2023]. The generalized nonparametric tests exhibit serious limitations across a wide range of scenarios. Although these methods sometimes lead to lower Type I error rates, this behavior is largely due to a substantial loss of statistical power when other effects are present. Consequently, we do not recommend the use of these methods.

## References {.unnumbered}

::: {#refs}
:::
